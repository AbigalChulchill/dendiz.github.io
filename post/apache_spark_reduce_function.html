<!DOCTYPE html>
<html lang="en">
  <head>
    <title> Apache spark reduce function</title>
      <meta charset="utf-8">
      <meta name="viewport" content="initial-scale=1">
      <link rel="stylesheet" href="/css/style.css">
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.18/dist/katex.min.css" integrity="sha384-zTROYFVGOfTw7JV7KUu8udsvW2fx4lWOsCEDqhBreBwlHI4ioVRtmIvEThzJHGET" crossorigin="anonymous">
      <script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.18/dist/katex.min.js" integrity="sha384-GxNFqL3r9uRJQhR+47eDxuPoNE7yLftQM8LcxzgS4HT73tp970WS/wV5p8UzCOmb" crossorigin="anonymous"></script>
      <script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.18/dist/contrib/auto-render.min.js" integrity="sha384-vZTG03m+2yp6N6BNi5iM4rW4oIwk5DfcNdFfxkk9ZWpDriOkXX8voJBFrAO7MpVl" crossorigin="anonymous"
           onload="renderMathInElement(document.body);"></script>
    </head>
    <body>
        <div>
        <strong>
        <a href="/micro/micro-index.html" onClick="this.href+='?rnd='+new Date().getTime()">Micro</a> &middot; 
        <a href="/post/index.html" onClick="this.href+='?rnd='+new Date().getTime()">Macro</a> &middot; 
        <a href="/post/about.html">About</a> &middot; 
        <a href="/post/now.html">Now</a> &middot; 
        <a href="/post/fn.html">fn</a>
        &middot;
        <a href="/twtxt.txt">twtxt</a>
        </strong>
        </div>
        <hr />
        <div id="main">
        <h1 id="title"> Apache spark reduce function</h1>
        <div id="date"><em>created  2015-05-26 </em></div>
        <div class="content">
        <p>normally reducing a list in scala will work as expected and you will get a value depending on your operation that is predictable.
for example consider the following list</p>
<pre><code class="language-scala">val x = List(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;)
</code></pre>
<p>and the operation we reduce is</p>
<pre><code class="language-scala">x.reduce((x,y) =&gt; x + y)
</code></pre>
<p>the accumulator will be &quot;a&quot; and the current element will be &quot;b&quot;. After the first iteration the accumulator will be &quot;ab&quot; and the current element will be &quot;c&quot; and in the final iteration the accumulator will be &quot;abc&quot;. Just the result we expected. But if you were to do this operation on an apache spark RDD you will get a different result each execution. Sometimes you will get &quot;abc&quot;, other times &quot;bca&quot;. You could end up with all variations. So what's going on here, why is the spark result unpredictable. It due to the parallelism that spark provides. How the partitions are aggregated are not defined. So when using the apache spark reduce you have to pay attention that the reducing function is commutative and also associative. Since string concatination is not we get a different result after each run. But if we were to define are list as a list of integers and use the same reduce function we would get the expected result each time, because the order of the numbers added does not change the result.</p>

        </div>
        </div>
	<footer>
	<em>generated on 2021-10-03 13:54:52</em>
	</footer>
    </body>
</html>
